# Defaults for finetuning with train.py.
#
# See go/t5x-finetune for instructions.
#
# You must also include a binding for MODEL, PROMPT, and PROMPT_LENGTHS.
#
# Required to be set:
#
# - MIXTURE_OR_TASK_NAME
# - TASK_FEATURE_LENGTHS
# - TRAIN_STEPS  # includes pretrain steps
# - MODEL_DIR  # automatically set when using xm_launch
# - INITIAL_CHECKPOINT_PATH
#
# When launching on XManager, `MODEL_DIR` (the directory to write fine-tuned
# checkpoints to) is configured automatically by the XManager launch script.
# When running locally, it needs to be passed in the `gin.MODEL_DIR` flag.
#
# `TRAIN_STEPS` should include pre-training steps, e.g., if pre-trained ckpt
# has 1M steps, TRAIN_STEPS = 1.1M will perform 0.1M fine-tuning steps.
#
# Commonly overridden options:
# - DROPOUT_RATE
# - BATCH_SIZE
# - PjitPartitioner.num_partitions
# - Trainer.num_microbatches
# - USE_CACHED_TASKS: Whether to look for preprocessed SeqIO data, or preprocess
#    on the fly. Most common tasks are cached, hence this is set to True by
#    default.
from __gin__ import dynamic_registration
from t5x import utils
from t5x import partitioning
from prompt_tuning.extended.train import multitask_partitioning as multitask_prompt_partitioning

include "t5x/configs/runs/finetune.gin"

# Prompt Tuning does not support packing.
train/utils.DatasetConfig.pack = False
train_eval/utils.DatasetConfig.pack = False

# ========== These are Prompt Tuning HPs you might want to override ==========
utils.create_learning_rate_scheduler:
  factors = "constant"
  base_learning_rate = 0.3

utils.SaveCheckpointConfig:
  period = 1000
  # Keep a single checkpoint. Even though the majority of these checkpoint
  # weights are unchanged from our initial checkpoint we keep the copy so
  # recovery from preemption works. We save our prompt values ourselves so we
  # don't have to worry about losing them.
  keep = 1

partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @multitask_prompt_partitioning.standard_logical_axis_rules()
