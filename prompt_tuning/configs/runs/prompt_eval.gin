# Defaults for eval.py.
#
#
# You must also include a binding for MODEL.
#
# Required to be set:
#
# - MIXTURE_OR_TASK_NAME: The SeqIO Task/Mixture to evaluate on
# - CHECKPOINT_PATH: The model checkpoint to evaluate
# - EVAL_OUTPUT_DIR: The dir to write results to.
# - PROMPT_FILE: The file to load the prompt from.
#
#
# Commonly overridden options:
#
# - DatasetConfig.split
# - DatasetConfig.batch_size
# - DatasetConfig.use_cached
# - RestoreCheckpointConfig.mode
# - PjitPartitioner.num_partitions
from __gin__ import dynamic_registration
import __main__ as eval_script
from t5x import partitioning
from prompt_tuning.train import partitioning as prompt_partitioning

include "t5x/configs/runs/eval.gin"
# Force loading a prompt from a file. If you want to evaluate a prompted model
# from the checkpoint produced by the prompt tuning training run (instead of
# loading from the checkpoint used to initialize prompt tuning + the learned
# prompt from a file that this config enables) you can use
# "t5x/configs/runs/infer.py" directly (you will probably need
# to update the RestoreCheckpointConfig to disable fallback_to_scratch used
# during training).
include "prompt_tuning/configs/prompts/from_file.gin"

# Enable "reinitialization" of parameters so the prompt will be initialized from
# file.
eval_script.evaluate.fallback_init_rng = 0

# Add partitioning rules for our new axis named `prompt`
partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @prompt_partitioning.standard_logical_axis_rules()
